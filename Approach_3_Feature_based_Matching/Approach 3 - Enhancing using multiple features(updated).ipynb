{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# Specify the connection details\n",
    "server = '172.16.3.22'\n",
    "database = 'patentview_trial'\n",
    "username = 'Raktim_Srivastava'\n",
    "password = 'IDSL2Sa!gt%1'\n",
    "\n",
    "# Create a connection string\n",
    "connection_string = f\"Driver={{SQL Server}};Server={server};Database={database};UID={username};PWD={password};\"\n",
    "\n",
    "# Establish the database connection\n",
    "connection = pyodbc.connect(connection_string)\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Execute the first SQL query\n",
    "query1 = \"\"\"\n",
    "SELECT\n",
    "    A.patent_id, A.inventor_sequence, A.inventor_id, A.disambig_inventor_name_first, A.disambig_inventor_name_last,\n",
    "    A.male_flag, A.location_id, B.disambig_city, B.disambig_state, B.disambig_country, B.latitude, B.longitude,\n",
    "    C.disambig_assignee_individual_name_first, C.disambig_assignee_individual_name_last, C.disambig_assignee_organization,\n",
    "    D.patent_type, D.patent_title, D.patent_abstract, D.num_claims,\n",
    "    CASE \n",
    "        WHEN SUBSTRING(F.cpc_class, 1, 1) = 'A' THEN 'Human Necessitates'\n",
    "        WHEN SUBSTRING(F.cpc_class, 1, 1) = 'B' THEN 'Performing Operations Transporting'\n",
    "        WHEN SUBSTRING(F.cpc_class, 1, 1) = 'C' THEN 'Chemistry Metallurgy'\n",
    "        WHEN SUBSTRING(F.cpc_class, 1, 1) = 'D' THEN 'Textiles Paper'\n",
    "        WHEN SUBSTRING(F.cpc_class, 1, 1) = 'E' THEN 'Fixed Constructions'\n",
    "        WHEN SUBSTRING(F.cpc_class, 1, 1) = 'F' THEN 'Mechanical Engineering Lighting Heating Weapons Blasting Engines or Pumps'\n",
    "        WHEN SUBSTRING(F.cpc_class, 1, 1) = 'G' THEN 'Physics'\n",
    "        WHEN SUBSTRING(F.cpc_class, 1, 1) = 'H' THEN 'Electricity'\n",
    "        WHEN SUBSTRING(F.cpc_class, 1, 1) = 'Y' THEN 'General Tagging of New Technological Developments'\n",
    "        ELSE F.cpc_class\n",
    "    END AS PATENT_CATEGORY,\n",
    "    F.cpc_subclass, F.cpc_subclass_title, F.cpc_group, F.cpc_group_title, F.cpc_class, F.cpc_class_title,\n",
    "    G.application_id, G.filing_date AS application_filing_date,\n",
    "    H.priority_claim_kind, H.foreign_application_id, H.filing_date, H.foreign_country_filed\n",
    "FROM dbo.CAPS_POC_USPTO_inventor_disambiguated A\n",
    "INNER JOIN [dbo].[USPTO_g_location_disambiguated_TT] B ON A.location_id = B.location_id\n",
    "LEFT JOIN [dbo].[USPTO_g_assignee_disambiguated_TT] C ON C.patent_id = A.patent_id\n",
    "LEFT JOIN [dbo].[USPTO_g_patent_TT] D ON D.patent_id = A.patent_id\n",
    "LEFT JOIN [dbo].USPTO_g_cpc_current_TT E ON E.patent_id = A.patent_id AND E.cpc_sequence = '0'\n",
    "LEFT JOIN [dbo].USPTO_g_cpc_title_TT F ON F.cpc_class = E.cpc_class AND F.cpc_subclass = E.cpc_subclass AND F.cpc_group = E.cpc_group\n",
    "LEFT JOIN [dbo].USPTO_g_application_TT G ON G.patent_id = A.patent_id\n",
    "LEFT JOIN [dbo].USPTO_g_foreign_priority_MAX_REC_TT H ON H.patent_id = A.patent_id\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query1)\n",
    "\n",
    "# Fetch the first query results\n",
    "results1 = cursor.fetchall()\n",
    "\n",
    "# Get the column names from the cursor description\n",
    "columns1 = [column[0] for column in cursor.description]\n",
    "\n",
    "# Convert the first query results to a DataFrame\n",
    "df1 = pd.DataFrame.from_records(results1, columns=columns1)\n",
    "\n",
    "# Execute the second SQL query\n",
    "query2 = \"\"\"\n",
    "SELECT\n",
    "    A.entity_id AS author_id, A.class AS Author_class, A.org_memberof, A.foaf_name AS Author_Name, A.paperCount,\n",
    "    A.paperFamilyCount, A.citationCount, B.paper AS Paper_Id, B.affiliation1, C.foaf_name AS Affliation_Name,\n",
    "    C.foundation_date, C.type_entities, C.pos_lat, C.pos_long, C.city_name, C.city_lat, C.city_lon, C.state_name,\n",
    "    C.postcode, C.country_name, C.country_alpha2, C.country_alpha3, C.country_official_name, D.class AS Paper_Class,\n",
    "    D.entity_type Paper_Entity_Type, D.appearsInJournal, D.estimatedCitationCount, D.referenceCount,\n",
    "    D.dcterms_created, D.dcterms_title, D.dcterms_publisher, D.dbo_publisher, F.foaf_name AS Field_of_Study_Name,\n",
    "    G.fos_list AS Field_list\n",
    "FROM [dbo].[CAPS_POC_EMAKG_Authors] A\n",
    "INNER JOIN [dbo].[EMAKG_PaperAuthorAffiliation_TT] B ON A.entity_id = B.author\n",
    "LEFT JOIN [dbo].[EMAKG_Affiliations_TT] C ON C.entity_id = B.affiliation1\n",
    "INNER JOIN [dbo].[EMAKG_Papers_TT] D ON D.entity_id = B.paper\n",
    "LEFT JOIN [dbo].[EMAKG_PaperFieldsOfStudy_TT] E ON E.entity_id = B.paper\n",
    "LEFT JOIN [dbo].[EMAKG_FieldsOfStudy_TT] F ON F.entity_id = E.fabio_hasDiscipline\n",
    "LEFT JOIN [dbo].[EMAKG_FieldOfStudyLabeled_TT] G ON G.entity_id = F.entity_id\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query2)\n",
    "\n",
    "# Fetch the second query results\n",
    "results2 = cursor.fetchall()\n",
    "\n",
    "# Get the column names from the cursor description\n",
    "columns2 = [column[0] for column in cursor.description]\n",
    "\n",
    "# Convert the second query results to a DataFrame\n",
    "df2 = pd.DataFrame.from_records(results2, columns=columns2)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Matching Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_id</th>\n",
       "      <th>inventor_sequence</th>\n",
       "      <th>inventor_id</th>\n",
       "      <th>disambig_inventor_name_first</th>\n",
       "      <th>disambig_inventor_name_last</th>\n",
       "      <th>male_flag</th>\n",
       "      <th>location_id</th>\n",
       "      <th>disambig_city</th>\n",
       "      <th>disambig_state</th>\n",
       "      <th>disambig_country</th>\n",
       "      <th>...</th>\n",
       "      <th>cpc_group</th>\n",
       "      <th>cpc_group_title</th>\n",
       "      <th>cpc_class</th>\n",
       "      <th>cpc_class_title</th>\n",
       "      <th>application_id</th>\n",
       "      <th>application_filing_date</th>\n",
       "      <th>priority_claim_kind</th>\n",
       "      <th>foreign_application_id</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>foreign_country_filed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000000</td>\n",
       "      <td>0</td>\n",
       "      <td>fl:jo_ln:marron-5</td>\n",
       "      <td>Joseph</td>\n",
       "      <td>Marron</td>\n",
       "      <td>1.0</td>\n",
       "      <td>198b0471-16c8-11ed-9b5f-1234bde3cd05</td>\n",
       "      <td>Manhattan Beach</td>\n",
       "      <td>CA</td>\n",
       "      <td>US</td>\n",
       "      <td>...</td>\n",
       "      <td>G01S7/4863</td>\n",
       "      <td>Details of systems according to groups G01S13/...</td>\n",
       "      <td>G01</td>\n",
       "      <td>MEASURING; TESTING</td>\n",
       "      <td>2015/14643719</td>\n",
       "      <td>2015-03-10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000001</td>\n",
       "      <td>1</td>\n",
       "      <td>fl:hy_ln:yu-30</td>\n",
       "      <td>Hyeon Jae</td>\n",
       "      <td>YU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3eb37495-16c8-11ed-9b5f-1234bde3cd05</td>\n",
       "      <td>Ansan-si</td>\n",
       "      <td>None</td>\n",
       "      <td>KR</td>\n",
       "      <td>...</td>\n",
       "      <td>B29C45/64</td>\n",
       "      <td>Injection moulding, i.e. forcing the required ...</td>\n",
       "      <td>B29</td>\n",
       "      <td>WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...</td>\n",
       "      <td>2015/14962323</td>\n",
       "      <td>2015-12-08</td>\n",
       "      <td>national</td>\n",
       "      <td>10-2014-0187779</td>\n",
       "      <td>2014-12-24</td>\n",
       "      <td>KR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000001</td>\n",
       "      <td>0</td>\n",
       "      <td>fl:su_ln:lee-389</td>\n",
       "      <td>Sun-Woo</td>\n",
       "      <td>Lee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6c4ba08f-16c8-11ed-9b5f-1234bde3cd05</td>\n",
       "      <td>Gunpo-si</td>\n",
       "      <td>None</td>\n",
       "      <td>KR</td>\n",
       "      <td>...</td>\n",
       "      <td>B29C45/64</td>\n",
       "      <td>Injection moulding, i.e. forcing the required ...</td>\n",
       "      <td>B29</td>\n",
       "      <td>WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...</td>\n",
       "      <td>2015/14962323</td>\n",
       "      <td>2015-12-08</td>\n",
       "      <td>national</td>\n",
       "      <td>10-2014-0187779</td>\n",
       "      <td>2014-12-24</td>\n",
       "      <td>KR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000002</td>\n",
       "      <td>2</td>\n",
       "      <td>fl:do_ln:choi-31</td>\n",
       "      <td>Dong-Hyeon</td>\n",
       "      <td>Choi</td>\n",
       "      <td>1.0</td>\n",
       "      <td>755a6338-16c8-11ed-9b5f-1234bde3cd05</td>\n",
       "      <td>Yongin-si</td>\n",
       "      <td>None</td>\n",
       "      <td>KR</td>\n",
       "      <td>...</td>\n",
       "      <td>B29C48/21</td>\n",
       "      <td>Extrusion moulding, i.e. expressing the mouldi...</td>\n",
       "      <td>B29</td>\n",
       "      <td>WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...</td>\n",
       "      <td>2014/15107519</td>\n",
       "      <td>2014-12-30</td>\n",
       "      <td>national</td>\n",
       "      <td>10-2013-0169223</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>KR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000002</td>\n",
       "      <td>3</td>\n",
       "      <td>fl:do_ln:kim-369</td>\n",
       "      <td>Dong Jin</td>\n",
       "      <td>Kim</td>\n",
       "      <td>1.0</td>\n",
       "      <td>755a6338-16c8-11ed-9b5f-1234bde3cd05</td>\n",
       "      <td>Yongin-si</td>\n",
       "      <td>None</td>\n",
       "      <td>KR</td>\n",
       "      <td>...</td>\n",
       "      <td>B29C48/21</td>\n",
       "      <td>Extrusion moulding, i.e. expressing the mouldi...</td>\n",
       "      <td>B29</td>\n",
       "      <td>WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...</td>\n",
       "      <td>2014/15107519</td>\n",
       "      <td>2014-12-30</td>\n",
       "      <td>national</td>\n",
       "      <td>10-2013-0169223</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>KR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  patent_id inventor_sequence        inventor_id disambig_inventor_name_first  \\\n",
       "0  10000000                 0  fl:jo_ln:marron-5                       Joseph   \n",
       "1  10000001                 1     fl:hy_ln:yu-30                    Hyeon Jae   \n",
       "2  10000001                 0   fl:su_ln:lee-389                      Sun-Woo   \n",
       "3  10000002                 2   fl:do_ln:choi-31                   Dong-Hyeon   \n",
       "4  10000002                 3   fl:do_ln:kim-369                     Dong Jin   \n",
       "\n",
       "  disambig_inventor_name_last  male_flag  \\\n",
       "0                      Marron        1.0   \n",
       "1                          YU        NaN   \n",
       "2                         Lee        NaN   \n",
       "3                        Choi        1.0   \n",
       "4                         Kim        1.0   \n",
       "\n",
       "                            location_id    disambig_city disambig_state  \\\n",
       "0  198b0471-16c8-11ed-9b5f-1234bde3cd05  Manhattan Beach             CA   \n",
       "1  3eb37495-16c8-11ed-9b5f-1234bde3cd05         Ansan-si           None   \n",
       "2  6c4ba08f-16c8-11ed-9b5f-1234bde3cd05         Gunpo-si           None   \n",
       "3  755a6338-16c8-11ed-9b5f-1234bde3cd05        Yongin-si           None   \n",
       "4  755a6338-16c8-11ed-9b5f-1234bde3cd05        Yongin-si           None   \n",
       "\n",
       "  disambig_country  ...   cpc_group  \\\n",
       "0               US  ...  G01S7/4863   \n",
       "1               KR  ...   B29C45/64   \n",
       "2               KR  ...   B29C45/64   \n",
       "3               KR  ...   B29C48/21   \n",
       "4               KR  ...   B29C48/21   \n",
       "\n",
       "                                     cpc_group_title cpc_class  \\\n",
       "0  Details of systems according to groups G01S13/...       G01   \n",
       "1  Injection moulding, i.e. forcing the required ...       B29   \n",
       "2  Injection moulding, i.e. forcing the required ...       B29   \n",
       "3  Extrusion moulding, i.e. expressing the mouldi...       B29   \n",
       "4  Extrusion moulding, i.e. expressing the mouldi...       B29   \n",
       "\n",
       "                                     cpc_class_title application_id  \\\n",
       "0                                 MEASURING; TESTING  2015/14643719   \n",
       "1  WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...  2015/14962323   \n",
       "2  WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...  2015/14962323   \n",
       "3  WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...  2014/15107519   \n",
       "4  WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...  2014/15107519   \n",
       "\n",
       "  application_filing_date priority_claim_kind foreign_application_id  \\\n",
       "0              2015-03-10                None                   None   \n",
       "1              2015-12-08            national        10-2014-0187779   \n",
       "2              2015-12-08            national        10-2014-0187779   \n",
       "3              2014-12-30            national        10-2013-0169223   \n",
       "4              2014-12-30            national        10-2013-0169223   \n",
       "\n",
       "   filing_date foreign_country_filed  \n",
       "0         None                  None  \n",
       "1   2014-12-24                    KR  \n",
       "2   2014-12-24                    KR  \n",
       "3   2013-12-31                    KR  \n",
       "4   2013-12-31                    KR  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the first and last names\n",
    "df1[\"full_name\"] = df1[\"disambig_inventor_name_first\"] + \" \" + df1[\"disambig_inventor_name_last\"]\n",
    "\n",
    "# Convert non-string values to strings in the 'full_name' column\n",
    "df1['full_name'] = df1['full_name'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Remove punctuation and spaces from the 'full_name' column\n",
    "df1['clean_name'] = df1['full_name'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x).replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_id</th>\n",
       "      <th>inventor_sequence</th>\n",
       "      <th>inventor_id</th>\n",
       "      <th>disambig_inventor_name_first</th>\n",
       "      <th>disambig_inventor_name_last</th>\n",
       "      <th>male_flag</th>\n",
       "      <th>location_id</th>\n",
       "      <th>disambig_city</th>\n",
       "      <th>disambig_state</th>\n",
       "      <th>disambig_country</th>\n",
       "      <th>...</th>\n",
       "      <th>cpc_class</th>\n",
       "      <th>cpc_class_title</th>\n",
       "      <th>application_id</th>\n",
       "      <th>application_filing_date</th>\n",
       "      <th>priority_claim_kind</th>\n",
       "      <th>foreign_application_id</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>foreign_country_filed</th>\n",
       "      <th>full_name</th>\n",
       "      <th>clean_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000000</td>\n",
       "      <td>0</td>\n",
       "      <td>fl:jo_ln:marron-5</td>\n",
       "      <td>Joseph</td>\n",
       "      <td>Marron</td>\n",
       "      <td>1.0</td>\n",
       "      <td>198b0471-16c8-11ed-9b5f-1234bde3cd05</td>\n",
       "      <td>Manhattan Beach</td>\n",
       "      <td>CA</td>\n",
       "      <td>US</td>\n",
       "      <td>...</td>\n",
       "      <td>G01</td>\n",
       "      <td>MEASURING; TESTING</td>\n",
       "      <td>2015/14643719</td>\n",
       "      <td>2015-03-10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Joseph Marron</td>\n",
       "      <td>JosephMarron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000001</td>\n",
       "      <td>1</td>\n",
       "      <td>fl:hy_ln:yu-30</td>\n",
       "      <td>Hyeon Jae</td>\n",
       "      <td>YU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3eb37495-16c8-11ed-9b5f-1234bde3cd05</td>\n",
       "      <td>Ansan-si</td>\n",
       "      <td>None</td>\n",
       "      <td>KR</td>\n",
       "      <td>...</td>\n",
       "      <td>B29</td>\n",
       "      <td>WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...</td>\n",
       "      <td>2015/14962323</td>\n",
       "      <td>2015-12-08</td>\n",
       "      <td>national</td>\n",
       "      <td>10-2014-0187779</td>\n",
       "      <td>2014-12-24</td>\n",
       "      <td>KR</td>\n",
       "      <td>Hyeon Jae YU</td>\n",
       "      <td>HyeonJaeYU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000001</td>\n",
       "      <td>0</td>\n",
       "      <td>fl:su_ln:lee-389</td>\n",
       "      <td>Sun-Woo</td>\n",
       "      <td>Lee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6c4ba08f-16c8-11ed-9b5f-1234bde3cd05</td>\n",
       "      <td>Gunpo-si</td>\n",
       "      <td>None</td>\n",
       "      <td>KR</td>\n",
       "      <td>...</td>\n",
       "      <td>B29</td>\n",
       "      <td>WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...</td>\n",
       "      <td>2015/14962323</td>\n",
       "      <td>2015-12-08</td>\n",
       "      <td>national</td>\n",
       "      <td>10-2014-0187779</td>\n",
       "      <td>2014-12-24</td>\n",
       "      <td>KR</td>\n",
       "      <td>Sun-Woo Lee</td>\n",
       "      <td>SunWooLee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000002</td>\n",
       "      <td>2</td>\n",
       "      <td>fl:do_ln:choi-31</td>\n",
       "      <td>Dong-Hyeon</td>\n",
       "      <td>Choi</td>\n",
       "      <td>1.0</td>\n",
       "      <td>755a6338-16c8-11ed-9b5f-1234bde3cd05</td>\n",
       "      <td>Yongin-si</td>\n",
       "      <td>None</td>\n",
       "      <td>KR</td>\n",
       "      <td>...</td>\n",
       "      <td>B29</td>\n",
       "      <td>WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...</td>\n",
       "      <td>2014/15107519</td>\n",
       "      <td>2014-12-30</td>\n",
       "      <td>national</td>\n",
       "      <td>10-2013-0169223</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>KR</td>\n",
       "      <td>Dong-Hyeon Choi</td>\n",
       "      <td>DongHyeonChoi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000002</td>\n",
       "      <td>3</td>\n",
       "      <td>fl:do_ln:kim-369</td>\n",
       "      <td>Dong Jin</td>\n",
       "      <td>Kim</td>\n",
       "      <td>1.0</td>\n",
       "      <td>755a6338-16c8-11ed-9b5f-1234bde3cd05</td>\n",
       "      <td>Yongin-si</td>\n",
       "      <td>None</td>\n",
       "      <td>KR</td>\n",
       "      <td>...</td>\n",
       "      <td>B29</td>\n",
       "      <td>WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...</td>\n",
       "      <td>2014/15107519</td>\n",
       "      <td>2014-12-30</td>\n",
       "      <td>national</td>\n",
       "      <td>10-2013-0169223</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>KR</td>\n",
       "      <td>Dong Jin Kim</td>\n",
       "      <td>DongJinKim</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  patent_id inventor_sequence        inventor_id disambig_inventor_name_first  \\\n",
       "0  10000000                 0  fl:jo_ln:marron-5                       Joseph   \n",
       "1  10000001                 1     fl:hy_ln:yu-30                    Hyeon Jae   \n",
       "2  10000001                 0   fl:su_ln:lee-389                      Sun-Woo   \n",
       "3  10000002                 2   fl:do_ln:choi-31                   Dong-Hyeon   \n",
       "4  10000002                 3   fl:do_ln:kim-369                     Dong Jin   \n",
       "\n",
       "  disambig_inventor_name_last  male_flag  \\\n",
       "0                      Marron        1.0   \n",
       "1                          YU        NaN   \n",
       "2                         Lee        NaN   \n",
       "3                        Choi        1.0   \n",
       "4                         Kim        1.0   \n",
       "\n",
       "                            location_id    disambig_city disambig_state  \\\n",
       "0  198b0471-16c8-11ed-9b5f-1234bde3cd05  Manhattan Beach             CA   \n",
       "1  3eb37495-16c8-11ed-9b5f-1234bde3cd05         Ansan-si           None   \n",
       "2  6c4ba08f-16c8-11ed-9b5f-1234bde3cd05         Gunpo-si           None   \n",
       "3  755a6338-16c8-11ed-9b5f-1234bde3cd05        Yongin-si           None   \n",
       "4  755a6338-16c8-11ed-9b5f-1234bde3cd05        Yongin-si           None   \n",
       "\n",
       "  disambig_country  ...  cpc_class  \\\n",
       "0               US  ...        G01   \n",
       "1               KR  ...        B29   \n",
       "2               KR  ...        B29   \n",
       "3               KR  ...        B29   \n",
       "4               KR  ...        B29   \n",
       "\n",
       "                                     cpc_class_title application_id  \\\n",
       "0                                 MEASURING; TESTING  2015/14643719   \n",
       "1  WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...  2015/14962323   \n",
       "2  WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...  2015/14962323   \n",
       "3  WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...  2014/15107519   \n",
       "4  WORKING OF PLASTICS; WORKING OF SUBSTANCES IN ...  2014/15107519   \n",
       "\n",
       "  application_filing_date priority_claim_kind foreign_application_id  \\\n",
       "0              2015-03-10                None                   None   \n",
       "1              2015-12-08            national        10-2014-0187779   \n",
       "2              2015-12-08            national        10-2014-0187779   \n",
       "3              2014-12-30            national        10-2013-0169223   \n",
       "4              2014-12-30            national        10-2013-0169223   \n",
       "\n",
       "  filing_date foreign_country_filed        full_name     clean_name  \n",
       "0        None                  None    Joseph Marron   JosephMarron  \n",
       "1  2014-12-24                    KR     Hyeon Jae YU     HyeonJaeYU  \n",
       "2  2014-12-24                    KR      Sun-Woo Lee      SunWooLee  \n",
       "3  2013-12-31                    KR  Dong-Hyeon Choi  DongHyeonChoi  \n",
       "4  2013-12-31                    KR     Dong Jin Kim     DongJinKim  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_name</th>\n",
       "      <th>disambig_country</th>\n",
       "      <th>disambig_city</th>\n",
       "      <th>disambig_assignee_organization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Marron</td>\n",
       "      <td>US</td>\n",
       "      <td>Manhattan Beach</td>\n",
       "      <td>Raytheon Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hyeon Jae YU</td>\n",
       "      <td>KR</td>\n",
       "      <td>Ansan-si</td>\n",
       "      <td>LS MTRON LTD.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sun-Woo Lee</td>\n",
       "      <td>KR</td>\n",
       "      <td>Gunpo-si</td>\n",
       "      <td>LS MTRON LTD.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dong-Hyeon Choi</td>\n",
       "      <td>KR</td>\n",
       "      <td>Yongin-si</td>\n",
       "      <td>KOLON INDUSTRIES. INC.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dong Jin Kim</td>\n",
       "      <td>KR</td>\n",
       "      <td>Yongin-si</td>\n",
       "      <td>KOLON INDUSTRIES. INC.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         full_name disambig_country    disambig_city  \\\n",
       "0    Joseph Marron               US  Manhattan Beach   \n",
       "1     Hyeon Jae YU               KR         Ansan-si   \n",
       "2      Sun-Woo Lee               KR         Gunpo-si   \n",
       "3  Dong-Hyeon Choi               KR        Yongin-si   \n",
       "4     Dong Jin Kim               KR        Yongin-si   \n",
       "\n",
       "  disambig_assignee_organization  \n",
       "0               Raytheon Company  \n",
       "1                  LS MTRON LTD.  \n",
       "2                  LS MTRON LTD.  \n",
       "3         KOLON INDUSTRIES. INC.  \n",
       "4         KOLON INDUSTRIES. INC.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_usp_final = df1[[\"full_name\",\"disambig_country\",\"disambig_city\",\"disambig_assignee_organization\"]]\n",
    "df_usp_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "df_usp_final = df_usp_final.rename(columns={\n",
    "    \"full_name\": \"name\",\n",
    "    \"disambig_country\": \"country\",\n",
    "    \"disambig_city\": \"city\",\n",
    "    \"Gender\" : \"gender\",\n",
    "    \"disambig_assignee_organization\" : \"affiliation\"\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Marron</td>\n",
       "      <td>US</td>\n",
       "      <td>Manhattan Beach</td>\n",
       "      <td>Raytheon Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hyeon Jae YU</td>\n",
       "      <td>KR</td>\n",
       "      <td>Ansan-si</td>\n",
       "      <td>LS MTRON LTD.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sun-Woo Lee</td>\n",
       "      <td>KR</td>\n",
       "      <td>Gunpo-si</td>\n",
       "      <td>LS MTRON LTD.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dong-Hyeon Choi</td>\n",
       "      <td>KR</td>\n",
       "      <td>Yongin-si</td>\n",
       "      <td>KOLON INDUSTRIES. INC.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dong Jin Kim</td>\n",
       "      <td>KR</td>\n",
       "      <td>Yongin-si</td>\n",
       "      <td>KOLON INDUSTRIES. INC.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name country             city             affiliation\n",
       "0    Joseph Marron      US  Manhattan Beach        Raytheon Company\n",
       "1     Hyeon Jae YU      KR         Ansan-si           LS MTRON LTD.\n",
       "2      Sun-Woo Lee      KR         Gunpo-si           LS MTRON LTD.\n",
       "3  Dong-Hyeon Choi      KR        Yongin-si  KOLON INDUSTRIES. INC.\n",
       "4     Dong Jin Kim      KR        Yongin-si  KOLON INDUSTRIES. INC."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making final df \n",
    "df_usp_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\drgun\\anaconda3\\lib\\site-packages (3.5.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (22.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (1.10.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 7.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (22.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             name country             city  \\\n",
      "0                   Joseph Marron      US  Manhattan Beach   \n",
      "1                    Hyeon Jae YU      KR         Ansan-si   \n",
      "2                     Sun-Woo Lee      KR         Gunpo-si   \n",
      "3                 Dong-Hyeon Choi      KR        Yongin-si   \n",
      "4                    Dong Jin Kim      KR        Yongin-si   \n",
      "5                      Si Min Kim      KR        Yongin-si   \n",
      "6                      Yun-Jo Kim      KR        Yongin-si   \n",
      "7                Carsten Elsasser      DE          Pulheim   \n",
      "8                 Cristoph Mehren      DE        Rübhausen   \n",
      "9                  Guido Bergmann      DE   Sankt Augustin   \n",
      "10        Jose Juan Valadez Lopez      MX        Monterrey   \n",
      "11  Miguel Jorge Zubiria Elizondo      MX        Monterrey   \n",
      "12                  Katsunori Oda      JP           Kariya   \n",
      "13                    Kenji Katou      JP           Kariya   \n",
      "14                    Marc Saelen      FR           Carvin   \n",
      "15                     Alex Huber      US        Milwaukee   \n",
      "16                  Corey Dickert      US       Brookfield   \n",
      "17                 Ian Zimmermann      US       West Allis   \n",
      "18                   Jarrod Kotes      US          Grafton   \n",
      "19                    Will Didier      US        Cedarburg   \n",
      "20                     Liah Caspi      US          Closter   \n",
      "21       Nathan Christopher Maier      US          Hayward   \n",
      "22                 Brian A. Hanna      US          Webster   \n",
      "23              Carlos M. Terrero      US          Ontario   \n",
      "24                 Donald R. Fess      US        Rochester   \n",
      "25                    Lynn Saxton      US         Walworth   \n",
      "26            Roberto A. Irizarry      US        Rochester   \n",
      "27            Gregory Thomas Mark      US        Cambridge   \n",
      "28                Carsten SANDNER      DE     Bad Dürkheim   \n",
      "29              Dietrich Scherzer      DE         Neustadt   \n",
      "30            Franz-Josef Dietzen      DE        Nümbrecht   \n",
      "31                 Herbert Schall      DE      Lichtenfels   \n",
      "32                  Tim DIEHLMANN      DE     Bad Kötzting   \n",
      "33                 Robert J. Nick      US        Pepperell   \n",
      "\n",
      "                            affiliation        NER_AFF  \n",
      "0                      Raytheon Company          [ORG]  \n",
      "1                         LS MTRON LTD.          [ORG]  \n",
      "2                         LS MTRON LTD.          [ORG]  \n",
      "3                KOLON INDUSTRIES. INC.          [ORG]  \n",
      "4                KOLON INDUSTRIES. INC.          [ORG]  \n",
      "5                KOLON INDUSTRIES. INC.          [ORG]  \n",
      "6                KOLON INDUSTRIES. INC.          [ORG]  \n",
      "7          KAUTEX TEXTRON GmbH & Co. KG  [ORG, PERSON]  \n",
      "8          KAUTEX TEXTRON GmbH & Co. KG  [ORG, PERSON]  \n",
      "9          KAUTEX TEXTRON GmbH & Co. KG  [ORG, PERSON]  \n",
      "10            ZUBEX INDUSTRIAL SA DE CV             []  \n",
      "11            ZUBEX INDUSTRIAL SA DE CV             []  \n",
      "12       TOYOTA SHATAI KABUSHIKI KAISHA  [ORG, PERSON]  \n",
      "13       TOYOTA SHATAI KABUSHIKI KAISHA  [ORG, PERSON]  \n",
      "14               REYDEL AUTOMOTIVE B.V.          [ORG]  \n",
      "15  MILWAUKEE ELECTRIC TOOL CORPORATION             []  \n",
      "16  MILWAUKEE ELECTRIC TOOL CORPORATION             []  \n",
      "17  MILWAUKEE ELECTRIC TOOL CORPORATION             []  \n",
      "18  MILWAUKEE ELECTRIC TOOL CORPORATION             []  \n",
      "19  MILWAUKEE ELECTRIC TOOL CORPORATION             []  \n",
      "20                       Alex Toys, LLC  [PERSON, ORG]  \n",
      "21                                 None             []  \n",
      "22                    Xerox Corporation       [PERSON]  \n",
      "23                    Xerox Corporation       [PERSON]  \n",
      "24                    Xerox Corporation       [PERSON]  \n",
      "25                    Xerox Corporation       [PERSON]  \n",
      "26                    Xerox Corporation       [PERSON]  \n",
      "27                     MARKFORGED, INC.          [ORG]  \n",
      "28                              BASF SE          [ORG]  \n",
      "29                              BASF SE          [ORG]  \n",
      "30                              BASF SE          [ORG]  \n",
      "31                              BASF SE          [ORG]  \n",
      "32                              BASF SE          [ORG]  \n",
      "33        Samsung Electronics Co., Ltd.          [ORG]  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def perform_ner(text):\n",
    "    if text is None:\n",
    "        return \"\"  # Replace None with an empty string\n",
    "    doc = nlp(text)\n",
    "    entities = [ ent.label_ for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "\n",
    "# Example usage\n",
    "column_name = 'affiliation'  # Replace with the column name containing the text\n",
    "\n",
    "\n",
    "# Apply NER to the specified column\n",
    "df_usp_final['NER_AFF'] = df_usp_final[column_name].fillna(\"\").apply(perform_ner)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the DataFrame with NER results\n",
    "print(df_usp_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>NER_AFF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Marron</td>\n",
       "      <td>US</td>\n",
       "      <td>Manhattan Beach</td>\n",
       "      <td>Raytheon Company</td>\n",
       "      <td>[ORG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hyeon Jae YU</td>\n",
       "      <td>KR</td>\n",
       "      <td>Ansan-si</td>\n",
       "      <td>LS MTRON LTD.</td>\n",
       "      <td>[ORG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sun-Woo Lee</td>\n",
       "      <td>KR</td>\n",
       "      <td>Gunpo-si</td>\n",
       "      <td>LS MTRON LTD.</td>\n",
       "      <td>[ORG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dong-Hyeon Choi</td>\n",
       "      <td>KR</td>\n",
       "      <td>Yongin-si</td>\n",
       "      <td>KOLON INDUSTRIES. INC.</td>\n",
       "      <td>[ORG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dong Jin Kim</td>\n",
       "      <td>KR</td>\n",
       "      <td>Yongin-si</td>\n",
       "      <td>KOLON INDUSTRIES. INC.</td>\n",
       "      <td>[ORG]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name country             city             affiliation NER_AFF\n",
       "0    Joseph Marron      US  Manhattan Beach        Raytheon Company   [ORG]\n",
       "1     Hyeon Jae YU      KR         Ansan-si           LS MTRON LTD.   [ORG]\n",
       "2      Sun-Woo Lee      KR         Gunpo-si           LS MTRON LTD.   [ORG]\n",
       "3  Dong-Hyeon Choi      KR        Yongin-si  KOLON INDUSTRIES. INC.   [ORG]\n",
       "4     Dong Jin Kim      KR        Yongin-si  KOLON INDUSTRIES. INC.   [ORG]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_usp_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG       21\n",
      "PERSON    11\n",
      "Name: NER_AFF, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ner_aff_counts = df_usp_final['NER_AFF'].explode().value_counts()\n",
    "print(ner_aff_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating City Clusters using Glove 840B Common Crawl Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Glove embeddings and Kmeans to create clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Step 1: Load pre-trained GloVe embeddings from pickle file\n",
    "def load_glove_embeddings(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        embeddings = pickle.load(file)\n",
    "    return embeddings\n",
    "\n",
    "# Step 2: Load the GloVe embeddings from the pickle file\n",
    "glove_embeddings = load_glove_embeddings('C:/Users/drgun/OneDrive/Desktop/Capstone/work/glove.840B.300d.pkl')\n",
    "\n",
    "\n",
    "# Step 3: Preprocess city names\n",
    "def preprocess_city_name(city_name):\n",
    "    # Perform any necessary preprocessing (e.g., lowercase, remove punctuation, handle special characters)\n",
    "    return city_name.lower()\n",
    "\n",
    "# Step 4: Compute city embeddings\n",
    "def get_city_embedding(city_name):\n",
    "    city_name = preprocess_city_name(city_name)\n",
    "    if city_name in glove_embeddings:\n",
    "        return glove_embeddings[city_name]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Step 5: Create city codes\n",
    "def create_city_codes(city_names, num_clusters):\n",
    "    city_embeddings = []\n",
    "    valid_city_names = []\n",
    "    for city_name in city_names:\n",
    "        embedding = get_city_embedding(city_name)\n",
    "        if embedding is not None:\n",
    "            city_embeddings.append(embedding)\n",
    "            valid_city_names.append(city_name)\n",
    "\n",
    "    # Perform clustering (K-means) if there are valid city names\n",
    "    if len(city_embeddings) > 0:\n",
    "        kmeans = KMeans(n_clusters=num_clusters)\n",
    "        kmeans.fit(city_embeddings)\n",
    "        labels = kmeans.labels_\n",
    "\n",
    "        city_codes = []\n",
    "        for city_name, label in zip(valid_city_names, labels):\n",
    "            city_codes.append((city_name, label))\n",
    "\n",
    "        return city_codes\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "# Step 6: Assign codes to city names\n",
    "def assign_codes(city_names, city_codes):\n",
    "    city_code_mapping = {}\n",
    "    for city_name, code in city_codes:\n",
    "        city_code_mapping[city_name] = code\n",
    "    return city_code_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drgun\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\drgun\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London 0\n",
      "Paris 0\n",
      "Tokyo 0\n",
      "Berlin 0\n",
      "Kanpur 1\n",
      "Malaysia 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "city_names = ['New York', 'London', 'Paris', 'Tokyo', 'Berlin','Kanpur','Malaysia']\n",
    "city_codes = create_city_codes(city_names, num_clusters=3)\n",
    "city_code_mapping = assign_codes(city_names, city_codes)\n",
    "\n",
    "# Print the city codes\n",
    "for city_name, city_code in city_code_mapping.items():\n",
    "    print(city_name, city_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Glove and DBSCAN to create clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\drgun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\drgun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "# Step 1: Load pre-trained GloVe embeddings from pickle file\n",
    "def load_glove_embeddings(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        embeddings = pickle.load(file)\n",
    "    return embeddings\n",
    "\n",
    "# Step 2: Load the GloVe embeddings from the pickle file\n",
    "glove_embeddings = load_glove_embeddings('C:/Users/drgun/OneDrive/Desktop/Capstone/work/glove.840B.300d.pkl')\n",
    "\n",
    "\n",
    "# Step 3: Preprocess city names\n",
    "def preprocess_city_name(city_name):\n",
    "    # Remove spaces and hyphens\n",
    "    city_name = city_name.replace(' ', '').replace('-', '')\n",
    "\n",
    "    # Remove punctuation\n",
    "    city_name = city_name.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove characters matching regex pattern\n",
    "    regex_pattern = r'[^\\w\\s]'\n",
    "    city_name = re.sub(regex_pattern, '', city_name)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    city_name = city_name.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    city_name = lemmatizer.lemmatize(city_name)\n",
    "\n",
    "    return city_name\n",
    "\n",
    "# Step 4: Compute city embeddings\n",
    "def get_city_embedding(city_name):\n",
    "    city_name = preprocess_city_name(city_name)\n",
    "    if city_name in glove_embeddings:\n",
    "        return glove_embeddings[city_name]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Step 5: Create city codes\n",
    "def create_city_codes(city_names, num_clusters):\n",
    "    city_embeddings = []\n",
    "    valid_city_names = []\n",
    "    for city_name in city_names:\n",
    "        embedding = get_city_embedding(city_name)\n",
    "        if embedding is not None:\n",
    "            city_embeddings.append(embedding)\n",
    "            valid_city_names.append(city_name)\n",
    "\n",
    "    # Perform clustering (K-means) if there are valid city names\n",
    "    if len(city_embeddings) > 0:\n",
    "        kmeans = KMeans(n_clusters=num_clusters)\n",
    "        kmeans.fit(city_embeddings)\n",
    "        labels = kmeans.labels_\n",
    "\n",
    "        city_codes = []\n",
    "        for city_name, label in zip(valid_city_names, labels):\n",
    "            city_codes.append((city_name, label))\n",
    "\n",
    "        return city_codes\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "# Step 6: Assign codes to city names\n",
    "def assign_codes(city_names, city_codes):\n",
    "    city_code_mapping = {}\n",
    "    for city_name, code in city_codes:\n",
    "        city_code_mapping[city_name] = code\n",
    "    return city_code_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drgun\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\drgun\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York 0\n",
      "London 0\n",
      "Paris 0\n",
      "Tokyo 0\n",
      "Berlin 0\n",
      "Kanpur 1\n",
      "Malaysia 0\n",
      "lucknow 1\n",
      "US 0\n",
      "Dubai 0\n",
      "USA 0\n",
      "United-States 2\n",
      "United States 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "city_names = ['New York', 'London', 'Paris', 'Tokyo', 'Berlin','Kanpur','Malaysia','lucknow', 'US','Dubai','USA', 'United-States',\"United States\"]\n",
    "city_codes = create_city_codes(city_names, num_clusters=3)\n",
    "city_code_mapping = assign_codes(city_names, city_codes)\n",
    "\n",
    "# Print the city codes\n",
    "for city_name, city_code in city_code_mapping.items():\n",
    "    print(city_name, city_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\drgun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\drgun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York -1\n",
      "London 0\n",
      "Paris 0\n",
      "Tokyo 0\n",
      "Berlin 0\n",
      "Kanpur 1\n",
      "Malaysia 0\n",
      "Lucknow 1\n",
      "US -1\n",
      "Dubai 0\n",
      "USA 0\n",
      "United-States 2\n",
      "United States 2\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Step 1: Load pre-trained GloVe embeddings from pickle file\n",
    "def load_glove_embeddings(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        embeddings = pickle.load(file)\n",
    "    return embeddings\n",
    "\n",
    "# Step 2: Load the GloVe embeddings from the pickle file\n",
    "glove_embeddings = load_glove_embeddings('C:/Users/drgun/OneDrive/Desktop/Capstone/work/glove.840B.300d.pkl')\n",
    "\n",
    "# Step 3: Preprocess city names\n",
    "# Step 3: Preprocess city names\n",
    "def preprocess_city_name(city_name):\n",
    "    # Remove spaces and hyphens\n",
    "    city_name = city_name.replace(' ', '').replace('-', '')\n",
    "\n",
    "    # Remove punctuation\n",
    "    city_name = city_name.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove characters matching regex pattern\n",
    "    regex_pattern = r'[^\\w\\s]'\n",
    "    city_name = re.sub(regex_pattern, '', city_name)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    city_name = city_name.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    city_name = lemmatizer.lemmatize(city_name)\n",
    "\n",
    "    return city_name\n",
    "\n",
    "# Step 4: Compute city embeddings\n",
    "def get_city_embedding(city_name):\n",
    "    city_name = preprocess_city_name(city_name)\n",
    "    if city_name in glove_embeddings:\n",
    "        return glove_embeddings[city_name]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Step 5: Create city codes using DBSCAN\n",
    "def create_city_codes(city_names, epsilon, min_samples):\n",
    "    city_embeddings = []\n",
    "    valid_city_names = []\n",
    "    for city_name in city_names:\n",
    "        embedding = get_city_embedding(city_name)\n",
    "        if embedding is not None:\n",
    "            city_embeddings.append(embedding)\n",
    "            valid_city_names.append(city_name)\n",
    "\n",
    "    # Perform clustering (DBSCAN) if there are valid city names\n",
    "    if len(city_embeddings) > 0:\n",
    "        dbscan = DBSCAN(eps=epsilon, min_samples=min_samples, metric='cosine')\n",
    "        dbscan.fit(city_embeddings)\n",
    "        labels = dbscan.labels_\n",
    "\n",
    "        city_codes = []\n",
    "        for city_name, label in zip(valid_city_names, labels):\n",
    "            city_codes.append((city_name, label))\n",
    "\n",
    "        return city_codes\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Step 6: Assign codes to city names\n",
    "def assign_codes(city_names, city_codes):\n",
    "    city_code_mapping = {}\n",
    "    for city_name, code in city_codes:\n",
    "        city_code_mapping[city_name] = code\n",
    "    return city_code_mapping\n",
    "\n",
    "# Example usage\n",
    "city_names = ['New York', 'London', 'Paris', 'Tokyo', 'Berlin', 'Kanpur', 'Malaysia', 'Lucknow', 'US', 'Dubai', 'USA', 'United-States', 'United States']\n",
    "city_codes = create_city_codes(city_names, epsilon=0.5, min_samples=2)\n",
    "city_code_mapping = assign_codes(city_names, city_codes)\n",
    "\n",
    "# Print the city codes\n",
    "for city_name, city_code in city_code_mapping.items():\n",
    "    print(city_name, city_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York -1\n",
      "London -1\n",
      "Paris -1\n",
      "Tokyo -1\n",
      "Berlin -1\n",
      "Kanpur -1\n",
      "Malaysia -1\n",
      "Lucknow -1\n",
      "US -1\n",
      "Dubai -1\n",
      "USA -1\n",
      "United-States 0\n",
      "United States 0\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "city_names = ['New York', 'London', 'Paris', 'Tokyo', 'Berlin', 'Kanpur', 'Malaysia', 'Lucknow', 'US', 'Dubai', 'USA', 'United-States', 'United States']\n",
    "city_codes = create_city_codes(city_names, epsilon=0.5, min_samples=2)\n",
    "city_code_mapping = assign_codes(city_names, city_codes)\n",
    "\n",
    "# Print the city codes\n",
    "for city_name, city_code in city_code_mapping.items():\n",
    "    print(city_name, city_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GLOVE and Agglomerative CLustering to create city clusters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the final code to approach as agglomerative performs the best when clustering and can be used for both City/Country for both datasets. The code also incorporates any missing city name or country names as well as embeddings where city names are not maatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\drgun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\drgun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York 0\n",
      "London 0\n",
      "Paris 0\n",
      "Tokyo 0\n",
      "Berlin 0\n",
      "Kanpur 4\n",
      "Malaysia 1\n",
      "Lucknow 4\n",
      "US 3\n",
      "Dubai 1\n",
      "USA 1\n",
      "United-States 2\n",
      "United States 2\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Step 1: Load pre-trained GloVe embeddings from pickle file\n",
    "def load_glove_embeddings(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        embeddings = pickle.load(file)\n",
    "    return embeddings\n",
    "\n",
    "# Step 2: Load the GloVe embeddings from the pickle file\n",
    "glove_embeddings = load_glove_embeddings('C:/Users/drgun/OneDrive/Desktop/Capstone/work/glove.840B.300d.pkl')\n",
    "\n",
    "# Step 3: Preprocess city names\n",
    "def preprocess_city_name(city_name):\n",
    "    if city_name is None:\n",
    "        return None\n",
    "\n",
    "    # Remove spaces and hyphens\n",
    "    city_name = city_name.replace(' ', '').replace('-', '')\n",
    " \n",
    "\n",
    "    # Remove punctuation\n",
    "    city_name = city_name.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove characters matching regex pattern\n",
    "    regex_pattern = r'[^\\w\\s]'\n",
    "    city_name = re.sub(regex_pattern, '', city_name)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    city_name = city_name.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    city_name = lemmatizer.lemmatize(city_name)\n",
    "\n",
    "    return city_name\n",
    "\n",
    "# Step 4: Compute city embeddings\n",
    "def get_city_embedding(city_name):\n",
    "    city_name = preprocess_city_name(city_name)\n",
    "    if city_name in glove_embeddings:\n",
    "        return glove_embeddings[city_name]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Step 5: Create city codes using hierarchical clustering\n",
    "def create_city_codes(city_names, num_clusters):\n",
    "    city_embeddings = []\n",
    "    valid_city_names = []\n",
    "    for city_name in city_names:\n",
    "        embedding = get_city_embedding(city_name)\n",
    "        if embedding is not None:\n",
    "            city_embeddings.append(embedding)\n",
    "            valid_city_names.append(city_name)\n",
    "\n",
    "    # Perform clustering (hierarchical clustering) if there are valid city names\n",
    "    if len(city_embeddings) > 0:\n",
    "        hierarchical_clustering = AgglomerativeClustering(n_clusters=num_clusters)\n",
    "        labels = hierarchical_clustering.fit_predict(city_embeddings)\n",
    "\n",
    "        city_codes = []\n",
    "        for city_name, label in zip(valid_city_names, labels):\n",
    "            city_codes.append((city_name, label))\n",
    "\n",
    "        return city_codes\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Step 6: Assign codes to city names\n",
    "def assign_codes(city_names, city_codes):\n",
    "    city_code_mapping = {}\n",
    "    for city_name, code in city_codes:\n",
    "        city_code_mapping[city_name] = code\n",
    "    return city_code_mapping\n",
    "\n",
    "# Example usage\n",
    "city_names = ['New York', 'London', 'Paris', 'Tokyo', 'Berlin', 'Kanpur', 'Malaysia', 'Lucknow', 'US', 'Dubai', 'USA', 'United-States', 'United States']\n",
    "city_codes = create_city_codes(city_names, num_clusters=5)\n",
    "city_code_mapping = assign_codes(city_names, city_codes)\n",
    "\n",
    "# Print the city codes\n",
    "for city_name, city_code in city_code_mapping.items():\n",
    "    print(city_name, city_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York 0\n",
      "London 0\n",
      "Paris 0\n",
      "Tokyo 0\n",
      "Berlin 0\n",
      "Kanpur 4\n",
      "Malaysia 1\n",
      "Lucknow 4\n",
      "US 3\n",
      "Dubai 1\n",
      "USA 1\n",
      "United-States 2\n",
      "United States 2\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "city_names = ['New York', 'London', 'Paris', 'Tokyo', 'Berlin', 'Kanpur', 'Malaysia', 'Lucknow', 'US', 'Dubai', 'USA', 'United-States', 'United States']\n",
    "city_codes = create_city_codes(city_names, num_clusters=5)\n",
    "city_code_mapping = assign_codes(city_names, city_codes)\n",
    "\n",
    "# Print the city codes\n",
    "for city_name, city_code in city_code_mapping.items():\n",
    "    print(city_name, city_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below code is to use the clustering to assign codes to final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>NER_AFF</th>\n",
       "      <th>CityCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Marron</td>\n",
       "      <td>US</td>\n",
       "      <td>Manhattan Beach</td>\n",
       "      <td>Raytheon Company</td>\n",
       "      <td>[ORG]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hyeon Jae YU</td>\n",
       "      <td>KR</td>\n",
       "      <td>Ansan-si</td>\n",
       "      <td>LS MTRON LTD.</td>\n",
       "      <td>[ORG]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sun-Woo Lee</td>\n",
       "      <td>KR</td>\n",
       "      <td>Gunpo-si</td>\n",
       "      <td>LS MTRON LTD.</td>\n",
       "      <td>[ORG]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dong-Hyeon Choi</td>\n",
       "      <td>KR</td>\n",
       "      <td>Yongin-si</td>\n",
       "      <td>KOLON INDUSTRIES. INC.</td>\n",
       "      <td>[ORG]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dong Jin Kim</td>\n",
       "      <td>KR</td>\n",
       "      <td>Yongin-si</td>\n",
       "      <td>KOLON INDUSTRIES. INC.</td>\n",
       "      <td>[ORG]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name country             city             affiliation NER_AFF  \\\n",
       "0    Joseph Marron      US  Manhattan Beach        Raytheon Company   [ORG]   \n",
       "1     Hyeon Jae YU      KR         Ansan-si           LS MTRON LTD.   [ORG]   \n",
       "2      Sun-Woo Lee      KR         Gunpo-si           LS MTRON LTD.   [ORG]   \n",
       "3  Dong-Hyeon Choi      KR        Yongin-si  KOLON INDUSTRIES. INC.   [ORG]   \n",
       "4     Dong Jin Kim      KR        Yongin-si  KOLON INDUSTRIES. INC.   [ORG]   \n",
       "\n",
       "   CityCode  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is called 'df' and the city column is 'City'\n",
    "\n",
    "# Extract the city names from the 'City' column\n",
    "city_names = df_usp_final['city'].tolist()\n",
    "\n",
    "# Apply the city clustering code to create city codes\n",
    "city_codes = create_city_codes(city_names, num_clusters=5)\n",
    "\n",
    "# Create a dictionary mapping city names to codes\n",
    "city_code_mapping = assign_codes(city_names, city_codes)\n",
    "\n",
    "# Create a new column 'CityCode' in the DataFrame\n",
    "df_usp_final['CityCode'] = df_usp_final['city'].map(city_code_mapping)\n",
    "\n",
    "# Print the DataFrame with the new 'CityCode' column\n",
    "df_usp_final.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below code helps check how many embeddings are matching in a particular column with Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Cities:\n",
      "Pulheim\n",
      "Ansan-si\n",
      "Rübhausen\n",
      "Sankt Augustin\n",
      "Nümbrecht\n",
      "Bad Kötzting\n",
      "West Allis\n",
      "Gunpo-si\n",
      "Bad Dürkheim\n",
      "Manhattan Beach\n",
      "Lichtenfels\n",
      "Yongin-si\n",
      "Present Cities:\n",
      "Grafton\n",
      "Webster\n",
      "Monterrey\n",
      "Closter\n",
      "Ontario\n",
      "Cedarburg\n",
      "Neustadt\n",
      "Walworth\n",
      "Kariya\n",
      "Milwaukee\n",
      "Rochester\n",
      "Cambridge\n",
      "Pepperell\n",
      "Carvin\n",
      "Hayward\n",
      "Brookfield\n",
      "Summary:\n",
      "Total cities: 28\n",
      "Missing cities: 12\n",
      "Present cities: 16\n"
     ]
    }
   ],
   "source": [
    "# Assuming your DataFrame is called 'df' and the city column is named 'City'\n",
    "city_names_df = df_usp_final['city'].tolist()\n",
    "\n",
    "# Set to store unique missing cities and present cities\n",
    "missing_cities = set()\n",
    "present_cities = set()\n",
    "\n",
    "# Iterate over the city names in the DataFrame and check if they are in the GloVe embeddings\n",
    "for city_name in city_names_df:\n",
    "    preprocessed_city_name = preprocess_city_name(city_name)\n",
    "    if preprocessed_city_name in glove_embeddings:\n",
    "        present_cities.add(city_name)\n",
    "    else:\n",
    "        missing_cities.add(city_name)\n",
    "\n",
    "# Print the missing cities\n",
    "print(\"Missing Cities:\")\n",
    "for city_name in missing_cities:\n",
    "    print(city_name)\n",
    "\n",
    "# Print the present cities\n",
    "print(\"Present Cities:\")\n",
    "for city_name in present_cities:\n",
    "    print(city_name)\n",
    "\n",
    "# Print summary\n",
    "print(\"Summary:\")\n",
    "print(\"Total cities:\", len(missing_cities)+len(present_cities))\n",
    "print(\"Missing cities:\", len(missing_cities))\n",
    "print(\"Present cities:\", len(present_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Cities:\n",
      "Present Cities:\n",
      "US\n",
      "JP\n",
      "FR\n",
      "MX\n",
      "KR\n",
      "DE\n",
      "Summary:\n",
      "Total cities: 6\n",
      "Missing cities: 0\n",
      "Present cities: 6\n"
     ]
    }
   ],
   "source": [
    "# Assuming your DataFrame is called 'df' and the city column is named 'City'\n",
    "city_names_df = df_usp_final['country'].tolist()\n",
    "\n",
    "# Set to store unique missing cities and present cities\n",
    "missing_cities = set()\n",
    "present_cities = set()\n",
    "\n",
    "# Iterate over the city names in the DataFrame and check if they are in the GloVe embeddings\n",
    "for city_name in city_names_df:\n",
    "    preprocessed_city_name = preprocess_city_name(city_name)\n",
    "    if preprocessed_city_name in glove_embeddings:\n",
    "        present_cities.add(city_name)\n",
    "    else:\n",
    "        missing_cities.add(city_name)\n",
    "\n",
    "# Print the missing cities\n",
    "print(\"Missing Cities:\")\n",
    "for city_name in missing_cities:\n",
    "    print(city_name)\n",
    "\n",
    "# Print the present cities\n",
    "print(\"Present Cities:\")\n",
    "for city_name in present_cities:\n",
    "    print(city_name)\n",
    "\n",
    "# Print summary\n",
    "print(\"Summary:\")\n",
    "print(\"Total cities:\", len(missing_cities)+len(present_cities))\n",
    "print(\"Missing cities:\", len(missing_cities))\n",
    "print(\"Present cities:\", len(present_cities))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above sample USPTO dataset As you can see 16 out of 12 city names are matching from Glove and all 6 country are present in Glove"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second DF_EMAKG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets repeat the same process for MAKG for Glove embeddings matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Query Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>Author_class</th>\n",
       "      <th>org_memberof</th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>paperCount</th>\n",
       "      <th>paperFamilyCount</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>Paper_Id</th>\n",
       "      <th>affiliation1</th>\n",
       "      <th>Affliation_Name</th>\n",
       "      <th>...</th>\n",
       "      <th>Paper_Entity_Type</th>\n",
       "      <th>appearsInJournal</th>\n",
       "      <th>estimatedCitationCount</th>\n",
       "      <th>referenceCount</th>\n",
       "      <th>dcterms_created</th>\n",
       "      <th>dcterms_title</th>\n",
       "      <th>dcterms_publisher</th>\n",
       "      <th>dbo_publisher</th>\n",
       "      <th>Field_of_Study_Name</th>\n",
       "      <th>Field_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2784860342</td>\n",
       "      <td>Author</td>\n",
       "      <td>241749</td>\n",
       "      <td>Joyce Jaeyun Kim</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2785120038</td>\n",
       "      <td>241749</td>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>Rooted Cosmopolitanism: A Theoretical Tool for...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2533942970</td>\n",
       "      <td>Author</td>\n",
       "      <td>241749</td>\n",
       "      <td>Dong-jin Kim</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1883392277</td>\n",
       "      <td>241749</td>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-06-24</td>\n",
       "      <td>Economic Reform in North Korea: A Dynamic Gene...</td>\n",
       "      <td>University Library of Munich, Germany</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2533942970</td>\n",
       "      <td>Author</td>\n",
       "      <td>241749</td>\n",
       "      <td>Dong-jin Kim</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2027719038</td>\n",
       "      <td>241749</td>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>...</td>\n",
       "      <td>JournalArticle</td>\n",
       "      <td>74096415.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-06-24</td>\n",
       "      <td>Potential economic reforms in North Korea: a d...</td>\n",
       "      <td>Taylor &amp; Francis Journals</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2274384076</td>\n",
       "      <td>Author</td>\n",
       "      <td>12834331</td>\n",
       "      <td>Dongjin Kim</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>2145237307</td>\n",
       "      <td>12834331</td>\n",
       "      <td>University of Wyoming</td>\n",
       "      <td>...</td>\n",
       "      <td>JournalArticle</td>\n",
       "      <td>24251679.0</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>2016-06-24</td>\n",
       "      <td>Low-storage Runge--Kutta methods for stochasti...</td>\n",
       "      <td>Elsevier Science Publishers B. V.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2274384076</td>\n",
       "      <td>Author</td>\n",
       "      <td>12834331</td>\n",
       "      <td>Dongjin Kim</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>2088605415</td>\n",
       "      <td>12834331</td>\n",
       "      <td>University of Wyoming</td>\n",
       "      <td>...</td>\n",
       "      <td>JournalArticle</td>\n",
       "      <td>148709879.0</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>2016-06-24</td>\n",
       "      <td>Numerical study of interacting particles appro...</td>\n",
       "      <td>Academic Press Professional, Inc.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id Author_class org_memberof       Author_Name  paperCount  \\\n",
       "0  2784860342       Author       241749  Joyce Jaeyun Kim           2   \n",
       "1  2533942970       Author       241749      Dong-jin Kim           2   \n",
       "2  2533942970       Author       241749      Dong-jin Kim           2   \n",
       "3  2274384076       Author     12834331       Dongjin Kim           2   \n",
       "4  2274384076       Author     12834331       Dongjin Kim           2   \n",
       "\n",
       "   paperFamilyCount  citationCount    Paper_Id  affiliation1  \\\n",
       "0                 2              1  2785120038        241749   \n",
       "1                 2              6  1883392277        241749   \n",
       "2                 2              6  2027719038        241749   \n",
       "3                 2             17  2145237307      12834331   \n",
       "4                 2             17  2088605415      12834331   \n",
       "\n",
       "           Affliation_Name  ...  Paper_Entity_Type appearsInJournal  \\\n",
       "0  University of Cambridge  ...               None              NaN   \n",
       "1  University of Cambridge  ...               None              NaN   \n",
       "2  University of Cambridge  ...     JournalArticle       74096415.0   \n",
       "3    University of Wyoming  ...     JournalArticle       24251679.0   \n",
       "4    University of Wyoming  ...     JournalArticle      148709879.0   \n",
       "\n",
       "   estimatedCitationCount  referenceCount dcterms_created  \\\n",
       "0                       0               0      2018-02-02   \n",
       "1                       1               4      2016-06-24   \n",
       "2                       5               3      2016-06-24   \n",
       "3                       5              21      2016-06-24   \n",
       "4                      12              22      2016-06-24   \n",
       "\n",
       "                                       dcterms_title  \\\n",
       "0  Rooted Cosmopolitanism: A Theoretical Tool for...   \n",
       "1  Economic Reform in North Korea: A Dynamic Gene...   \n",
       "2  Potential economic reforms in North Korea: a d...   \n",
       "3  Low-storage Runge--Kutta methods for stochasti...   \n",
       "4  Numerical study of interacting particles appro...   \n",
       "\n",
       "                       dcterms_publisher dbo_publisher Field_of_Study_Name  \\\n",
       "0                                   None          None                None   \n",
       "1  University Library of Munich, Germany          None                None   \n",
       "2              Taylor & Francis Journals          None                None   \n",
       "3      Elsevier Science Publishers B. V.          None                None   \n",
       "4      Academic Press Professional, Inc.          None                None   \n",
       "\n",
       "  Field_list  \n",
       "0       None  \n",
       "1       None  \n",
       "2       None  \n",
       "3       None  \n",
       "4       None  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the second DataFrame\n",
    "print(\"Second Query Results:\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "# Remove punctuation and spaces from the 'full_name' column\n",
    "df2['clean_name'] = df2['Author_Name'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x).replace(\" \", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>country_alpha2</th>\n",
       "      <th>city_name</th>\n",
       "      <th>Affliation_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joyce Jaeyun Kim</td>\n",
       "      <td>GB</td>\n",
       "      <td>East of England</td>\n",
       "      <td>University of Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dong-jin Kim</td>\n",
       "      <td>GB</td>\n",
       "      <td>East of England</td>\n",
       "      <td>University of Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dong-jin Kim</td>\n",
       "      <td>GB</td>\n",
       "      <td>East of England</td>\n",
       "      <td>University of Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dongjin Kim</td>\n",
       "      <td>US</td>\n",
       "      <td>Albany County</td>\n",
       "      <td>University of Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dongjin Kim</td>\n",
       "      <td>US</td>\n",
       "      <td>Albany County</td>\n",
       "      <td>University of Wyoming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Author_Name country_alpha2        city_name          Affliation_Name\n",
       "0  Joyce Jaeyun Kim             GB  East of England  University of Cambridge\n",
       "1      Dong-jin Kim             GB  East of England  University of Cambridge\n",
       "2      Dong-jin Kim             GB  East of England  University of Cambridge\n",
       "3       Dongjin Kim             US    Albany County    University of Wyoming\n",
       "4       Dongjin Kim             US    Albany County    University of Wyoming"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Subsetting required columns\n",
    "df_makg_final = df2[[\"Author_Name\",\"country_alpha2\",\"city_name\",'Affliation_Name']]\n",
    "df_makg_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "df_makg_final = df_makg_final.rename(columns={\n",
    "    \"Author_Name\": \"name\",\n",
    "    \"country_alpha2\": \"country\",\n",
    "    \"city_name\": \"city\",\n",
    "    \"Gender\" : \"gender\",\n",
    "    \"Affliation_Name\":\"affiliation\"\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joyce Jaeyun Kim</td>\n",
       "      <td>GB</td>\n",
       "      <td>East of England</td>\n",
       "      <td>University of Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dong-jin Kim</td>\n",
       "      <td>GB</td>\n",
       "      <td>East of England</td>\n",
       "      <td>University of Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dong-jin Kim</td>\n",
       "      <td>GB</td>\n",
       "      <td>East of England</td>\n",
       "      <td>University of Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dongjin Kim</td>\n",
       "      <td>US</td>\n",
       "      <td>Albany County</td>\n",
       "      <td>University of Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dongjin Kim</td>\n",
       "      <td>US</td>\n",
       "      <td>Albany County</td>\n",
       "      <td>University of Wyoming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name country             city              affiliation\n",
       "0  Joyce Jaeyun Kim      GB  East of England  University of Cambridge\n",
       "1      Dong-jin Kim      GB  East of England  University of Cambridge\n",
       "2      Dong-jin Kim      GB  East of England  University of Cambridge\n",
       "3       Dongjin Kim      US    Albany County    University of Wyoming\n",
       "4       Dongjin Kim      US    Albany County    University of Wyoming"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_makg_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_city_name(city_name):\n",
    "    if city_name is None:\n",
    "        return None\n",
    "\n",
    "    # Remove spaces and hyphens\n",
    "    #city_name = city_name.replace(' ', '').replace('-', '')\n",
    " \n",
    "\n",
    "    # Remove punctuation\n",
    "    #city_name = city_name.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove characters matching regex pattern\n",
    "    #regex_pattern = r'[^\\w\\s]'\n",
    "    #city_name = re.sub(regex_pattern, '', city_name)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    #city_name = city_name.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #city_name = lemmatizer.lemmatize(city_name)\n",
    "\n",
    "    return city_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Cities:\n",
      "Present Cities:\n",
      "Braunschweig\n",
      "Chemnitz\n",
      "Tuebingen\n",
      "Taipei\n",
      "Kumamoto\n",
      "Geneve\n",
      "Petaling\n",
      "Ulm\n",
      "Donnybrook\n",
      "Darmstadt\n",
      "Mitte\n",
      "Alexandria\n",
      "Sapporo\n",
      "Stadtlohn\n",
      "Changwon\n",
      "Nonsan\n",
      "Chinju\n",
      "Changning\n",
      "Uberlandia\n",
      "Wroclaw\n",
      "Washington\n",
      "Chavannes\n",
      "Karlsruhe\n",
      "Qinghe\n",
      "Chuncheon\n",
      "Goettingen\n",
      "Mountainview\n",
      "Freiburg\n",
      "Innsbruck\n",
      "Villigen\n",
      "Iksan\n",
      "Gwangju\n",
      "Calgary\n",
      "Seoul\n",
      "Jona\n",
      "Dresden\n",
      "Rolle\n",
      "Glendale\n",
      "Baltimore\n",
      "Jonkoping\n",
      "Siena\n",
      "Dahlem\n",
      "Incheon\n",
      "Matsuyama\n",
      "Paris\n",
      "Linz\n",
      "Cheonan\n",
      "Asan\n",
      "Londrina\n",
      "Ibaraki\n",
      "Aachen\n",
      "Bonn\n",
      "Eisen\n",
      "Jeddah\n",
      "Daejeon\n",
      "Boeblingen\n",
      "Daegu\n",
      "Naju\n",
      "Ulsan\n",
      "Wuerzburg\n",
      "Sanaa\n",
      "Heidelberg\n",
      "Essonne\n",
      "Hayama\n",
      "Kimhae\n",
      "Buch\n",
      "Berkeley\n",
      "Busan\n",
      "Erlangen\n",
      "Malibu\n",
      "Tokyo\n",
      "Kiel\n",
      "Passau\n",
      "Laatzen\n",
      "Ecublens\n",
      "Sendai\n",
      "Halifax\n",
      "Wonju\n",
      "Singapore\n",
      "Dublin\n",
      "Norfolk\n",
      "Suncheon\n",
      "Alsergrund\n",
      "Hauts-de-Seine\n",
      "Bern\n",
      "Schwieberdingen\n",
      "Machida\n",
      "Giessen\n",
      "Laguna\n",
      "Schaffhausen\n",
      "Takatsuki\n",
      "Ingolstadt\n",
      "Naka\n",
      "Luebeck\n",
      "Campinas\n",
      "Gongju\n",
      "Pohang\n",
      "Munich\n",
      "Brussels\n",
      "Julich\n",
      "Richmond\n",
      "Sandnes\n",
      "Kunsan\n",
      "Gumi\n",
      "Turku\n",
      "Roscommon\n",
      "Dilbeek\n",
      "Yangsan\n",
      "Anseong\n",
      "Riga\n",
      "Summary:\n",
      "Total cities: 325\n",
      "Missing cities: 215\n",
      "Present cities: 110\n"
     ]
    }
   ],
   "source": [
    "# Assuming your DataFrame is called 'df' and the city column is named 'City'\n",
    "city_names_df = df_makg_final['city'].tolist()\n",
    "\n",
    "# Set to store unique missing cities and present cities\n",
    "missing_cities = set()\n",
    "present_cities = set()\n",
    "\n",
    "# Iterate over the city names in the DataFrame and check if they are in the GloVe embeddings\n",
    "for city_name in city_names_df:\n",
    "    preprocessed_city_name = preprocess_city_name(city_name)\n",
    "    if preprocessed_city_name in glove_embeddings:\n",
    "        present_cities.add(city_name)\n",
    "    else:\n",
    "        missing_cities.add(city_name)\n",
    "\n",
    "# Print the missing cities\n",
    "print(\"Missing Cities:\")\n",
    "#for city_name in missing_cities:\n",
    "    #print(city_name)\n",
    "\n",
    "# Print the present cities\n",
    "print(\"Present Cities:\")\n",
    "for city_name in present_cities:\n",
    "    print(city_name)\n",
    "\n",
    "# Print summary\n",
    "print(\"Summary:\")\n",
    "print(\"Total cities:\", len(missing_cities)+len(present_cities))\n",
    "print(\"Missing cities:\", len(missing_cities))\n",
    "print(\"Present cities:\", len(present_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Cities:\n",
      "None\n",
      "Present Cities:\n",
      "MY\n",
      "BR\n",
      "CZ\n",
      "GB\n",
      "SG\n",
      "IE\n",
      "FR\n",
      "HK\n",
      "JP\n",
      "TZ\n",
      "SE\n",
      "CH\n",
      "BE\n",
      "ES\n",
      "IL\n",
      "SA\n",
      "US\n",
      "AT\n",
      "LV\n",
      "NO\n",
      "ZA\n",
      "PH\n",
      "IN\n",
      "AU\n",
      "CA\n",
      "KE\n",
      "NZ\n",
      "CO\n",
      "IT\n",
      "CY\n",
      "PL\n",
      "KR\n",
      "YE\n",
      "TW\n",
      "CN\n",
      "FI\n",
      "VN\n",
      "BT\n",
      "DK\n",
      "DE\n",
      "KM\n",
      "AE\n",
      "Summary:\n",
      "Total cities: 43\n",
      "Missing cities: 1\n",
      "Present cities: 42\n"
     ]
    }
   ],
   "source": [
    "# Assuming your DataFrame is called 'df' and the city column is named 'City'\n",
    "city_names_df = df_makg_final['country'].tolist()\n",
    "\n",
    "# Set to store unique missing cities and present cities\n",
    "missing_cities = set()\n",
    "present_cities = set()\n",
    "\n",
    "# Iterate over the city names in the DataFrame and check if they are in the GloVe embeddings\n",
    "for city_name in city_names_df:\n",
    "    preprocessed_city_name = preprocess_city_name(city_name)\n",
    "    if preprocessed_city_name in glove_embeddings:\n",
    "        present_cities.add(city_name)\n",
    "    else:\n",
    "        missing_cities.add(city_name)\n",
    "\n",
    "# Print the missing cities\n",
    "print(\"Missing Cities:\")\n",
    "for city_name in missing_cities:\n",
    "    print(city_name)\n",
    "\n",
    "# Print the present cities\n",
    "print(\"Present Cities:\")\n",
    "for city_name in present_cities:\n",
    "    print(city_name)\n",
    "\n",
    "# Print summary\n",
    "print(\"Summary:\")\n",
    "print(\"Total cities:\", len(missing_cities)+len(present_cities))\n",
    "print(\"Missing cities:\", len(missing_cities))\n",
    "print(\"Present cities:\", len(present_cities))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see for MAKG there are 110 cities present in embeddings while remaining 215 are missing  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below COde to assign clusters to both data frames with similar codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique city names DataFrame from both DataFrames\n",
    "unique_city_names = pd.concat([df_usp_final['city'], df_makg_final['city']]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Generate city codes based on the unique city names\n",
    "city_codes = create_city_codes(unique_city_names, num_clusters=6)\n",
    "\n",
    "# Create a mapping of city names to codes\n",
    "city_code_mapping = dict(city_codes)\n",
    "\n",
    "# Apply city code mapping to both DataFrames\n",
    "df_usp_final['CityCode'] = df_usp_final['city'].map(city_code_mapping)\n",
    "df_makg_final['CityCode'] = df_makg_final['city'].map(city_code_mapping)\n",
    "\n",
    "# Print the DataFrames with city codes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below code to assign country codes basis clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique city names DataFrame from both DataFrames\n",
    "unique_city_names = pd.concat([df_usp_final['country'], df_makg_final['country']]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Generate city codes based on the unique city names\n",
    "city_codes = create_city_codes(unique_city_names, num_clusters=6)\n",
    "\n",
    "# Create a mapping of city names to codes\n",
    "city_code_mapping = dict(city_codes)\n",
    "\n",
    "# Apply city code mapping to both DataFrames\n",
    "df_usp_final['CountryCode'] = df_usp_final['country'].map(city_code_mapping)\n",
    "df_makg_final['CountryCode'] = df_makg_final['country'].map(city_code_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CityCodeSummary\n",
      "21\n",
      "9420\n",
      "CountryCodeSummary\n",
      "34\n",
      "22964\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>CityCode</th>\n",
       "      <th>CountryCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joyce Jaeyun Kim</td>\n",
       "      <td>GB</td>\n",
       "      <td>East of England</td>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dong-jin Kim</td>\n",
       "      <td>GB</td>\n",
       "      <td>East of England</td>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dong-jin Kim</td>\n",
       "      <td>GB</td>\n",
       "      <td>East of England</td>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dongjin Kim</td>\n",
       "      <td>US</td>\n",
       "      <td>Albany County</td>\n",
       "      <td>University of Wyoming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dongjin Kim</td>\n",
       "      <td>US</td>\n",
       "      <td>Albany County</td>\n",
       "      <td>University of Wyoming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name country             city              affiliation  \\\n",
       "0  Joyce Jaeyun Kim      GB  East of England  University of Cambridge   \n",
       "1      Dong-jin Kim      GB  East of England  University of Cambridge   \n",
       "2      Dong-jin Kim      GB  East of England  University of Cambridge   \n",
       "3       Dongjin Kim      US    Albany County    University of Wyoming   \n",
       "4       Dongjin Kim      US    Albany County    University of Wyoming   \n",
       "\n",
       "   CityCode  CountryCode  \n",
       "0       NaN          0.0  \n",
       "1       NaN          0.0  \n",
       "2       NaN          0.0  \n",
       "3       NaN          0.0  \n",
       "4       NaN          0.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking if its assigng some codes\n",
    "print(\"CityCodeSummary\")\n",
    "print(df_usp_final['CityCode'].count())\n",
    "print(df_makg_final['CityCode'].count())\n",
    "\n",
    "print(\"CountryCodeSummary\")\n",
    "print(df_usp_final['CountryCode'].count())\n",
    "print(df_makg_final['CountryCode'].count())\n",
    "\n",
    "\n",
    "\n",
    "df_makg_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below code to assign NER codes using SPACY for affiliation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def perform_ner(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.label_ for ent in doc.ents]\n",
    "    if entities:\n",
    "        return entities[0]  # Return the first entity label\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Example usage\n",
    "column_name = 'affiliation'  # Replace with the column name containing the text\n",
    "\n",
    "\n",
    "# Apply NER to the specified column\n",
    "df_usp_final['NER_AFF'] = df_usp_final[column_name].fillna(\"\").apply(perform_ner)\n",
    "df_makg_final['NER_AFF'] = df_usp_final[column_name].fillna(\"\").apply(perform_ner)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG       20\n",
      "           8\n",
      "PERSON     6\n",
      "Name: NER_AFF, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ner_aff_counts_usp = df_usp_final['NER_AFF'].explode().value_counts()\n",
    "\n",
    "print(ner_aff_counts_usp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>NER_AFF</th>\n",
       "      <th>CityCode</th>\n",
       "      <th>CountryCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joseph Marron</td>\n",
       "      <td>US</td>\n",
       "      <td>Manhattan Beach</td>\n",
       "      <td>Raytheon Company</td>\n",
       "      <td>ORG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hyeon Jae YU</td>\n",
       "      <td>KR</td>\n",
       "      <td>Ansan-si</td>\n",
       "      <td>LS MTRON LTD.</td>\n",
       "      <td>ORG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sun-Woo Lee</td>\n",
       "      <td>KR</td>\n",
       "      <td>Gunpo-si</td>\n",
       "      <td>LS MTRON LTD.</td>\n",
       "      <td>ORG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dong-Hyeon Choi</td>\n",
       "      <td>KR</td>\n",
       "      <td>Yongin-si</td>\n",
       "      <td>KOLON INDUSTRIES. INC.</td>\n",
       "      <td>ORG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dong Jin Kim</td>\n",
       "      <td>KR</td>\n",
       "      <td>Yongin-si</td>\n",
       "      <td>KOLON INDUSTRIES. INC.</td>\n",
       "      <td>ORG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name country             city             affiliation NER_AFF  \\\n",
       "0    Joseph Marron      US  Manhattan Beach        Raytheon Company     ORG   \n",
       "1     Hyeon Jae YU      KR         Ansan-si           LS MTRON LTD.     ORG   \n",
       "2      Sun-Woo Lee      KR         Gunpo-si           LS MTRON LTD.     ORG   \n",
       "3  Dong-Hyeon Choi      KR        Yongin-si  KOLON INDUSTRIES. INC.     ORG   \n",
       "4     Dong Jin Kim      KR        Yongin-si  KOLON INDUSTRIES. INC.     ORG   \n",
       "\n",
       "   CityCode  CountryCode  \n",
       "0       NaN            0  \n",
       "1       NaN            1  \n",
       "2       NaN            1  \n",
       "3       NaN            1  \n",
       "4       NaN            1  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_usp_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>CityCode</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>NER_AFF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joyce Jaeyun Kim</td>\n",
       "      <td>GB</td>\n",
       "      <td>East of England</td>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dong-jin Kim</td>\n",
       "      <td>GB</td>\n",
       "      <td>East of England</td>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dong-jin Kim</td>\n",
       "      <td>GB</td>\n",
       "      <td>East of England</td>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dongjin Kim</td>\n",
       "      <td>US</td>\n",
       "      <td>Albany County</td>\n",
       "      <td>University of Wyoming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dongjin Kim</td>\n",
       "      <td>US</td>\n",
       "      <td>Albany County</td>\n",
       "      <td>University of Wyoming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name country             city              affiliation  \\\n",
       "0  Joyce Jaeyun Kim      GB  East of England  University of Cambridge   \n",
       "1      Dong-jin Kim      GB  East of England  University of Cambridge   \n",
       "2      Dong-jin Kim      GB  East of England  University of Cambridge   \n",
       "3       Dongjin Kim      US    Albany County    University of Wyoming   \n",
       "4       Dongjin Kim      US    Albany County    University of Wyoming   \n",
       "\n",
       "   CityCode  CountryCode NER_AFF  \n",
       "0       NaN          0.0     ORG  \n",
       "1       NaN          0.0     ORG  \n",
       "2       NaN          0.0     ORG  \n",
       "3       NaN          0.0     ORG  \n",
       "4       NaN          0.0     ORG  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_makg_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: recordlinkage in c:\\users\\drgun\\anaconda3\\lib\\site-packages (0.15)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from recordlinkage) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.13.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from recordlinkage) (1.23.5)\n",
      "Requirement already satisfied: jellyfish>=0.8.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from recordlinkage) (0.9.0)\n",
      "Requirement already satisfied: pandas<2,>=1 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from recordlinkage) (1.5.3)\n",
      "Requirement already satisfied: scipy>=1 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from recordlinkage) (1.10.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from recordlinkage) (1.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from pandas<2,>=1->recordlinkage) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from pandas<2,>=1->recordlinkage) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.0->recordlinkage) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\drgun\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas<2,>=1->recordlinkage) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install recordlinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recordlinkage\n",
    "import pandas as pd\n",
    "\n",
    "df_usp_final = df_usp_final.fillna('')\n",
    "df_makg_final = df_makg_final.fillna('')\n",
    "\n",
    "\n",
    "columns_to_convert = ['name', 'gender', 'country', 'city', 'affiliation']\n",
    "df_usp_final[columns_to_convert] = df_usp_final[columns_to_convert].astype(str)\n",
    "df_makg_final[columns_to_convert] = df_makg_final[columns_to_convert].astype(str)\n",
    "\n",
    "\n",
    "# Step 2: Define indexing\n",
    "indexer = recordlinkage.Index()\n",
    "indexer.block('name')\n",
    "indexer.sortedneighbourhood('country')\n",
    "pairs = indexer.index(df_usp_final,df_makg_final)\n",
    "\n",
    "# Step 3: Define comparison\n",
    "compare = recordlinkage.Compare()\n",
    "compare.string('name', 'name', method='jarowinkler', threshold=0.8)\n",
    "compare.string('gender', 'gender', method='jarowinkler', threshold=0.8)\n",
    "compare.string('country', 'country', method='jarowinkler', threshold=0.8)\n",
    "compare.string('city', 'city', method='jarowinkler', threshold=0.8)\n",
    "compare.string('affiliation', 'affiliation', method='jarowinkler', threshold=0.8)\n",
    "similarity = compare.compute(pairs, df_usp_final,df_makg_final)\n",
    "\n",
    "# Step 4: Perform classification\n",
    "matches = recordlinkage.KMeansClassifier().fit_predict(similarity)\n",
    "\n",
    "# Step 5: Retrieve matched record pairs\n",
    "matched_pairs = pairs[matches == 1]\n",
    "\n",
    "# Step 6: Create DataFrame or write results to a file\n",
    "matched_records = pd.DataFrame(matched_pairs, columns=['ID_1', 'ID_2'])\n",
    "matched_records.to_csv('matched_records.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recordlinkage\n",
    "import pandas as pd\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "df_usp_final = df_usp_final.fillna('')\n",
    "df_makg_final = df_makg_final.fillna('')\n",
    "\n",
    "# Convert columns to string type\n",
    "columns_to_convert = ['name', 'gender', 'country', 'city', 'affiliation']\n",
    "df_usp_final[columns_to_convert] = df_usp_final[columns_to_convert].astype(str)\n",
    "df_makg_final[columns_to_convert] = df_makg_final[columns_to_convert].astype(str)\n",
    "\n",
    "# Define indexing\n",
    "indexer = recordlinkage.Index()\n",
    "indexer.block('name')\n",
    "indexer.sortedneighbourhood('country')\n",
    "pairs = indexer.index(df_usp_final, df_makg_final)\n",
    "\n",
    "# Define comparison\n",
    "compare = recordlinkage.Compare()\n",
    "compare.string('name', 'name', method='jarowinkler', threshold=0.1)\n",
    "compare.string('gender', 'gender', method='jarowinkler', threshold=0.1)\n",
    "compare.string('country', 'country', method='jarowinkler', threshold=0.1)\n",
    "compare.string('city', 'city', method='jarowinkler', threshold=0.1)\n",
    "compare.string('affiliation', 'affiliation', method='jarowinkler', threshold=0.1)\n",
    "similarity = compare.compute(pairs, df_usp_final, df_makg_final)\n",
    "\n",
    "# Perform classification\n",
    "matches = recordlinkage.KMeansClassifier().fit_predict(similarity)\n",
    "\n",
    "# Retrieve matched record pairs\n",
    "matched_pairs = pairs[matches.values]\n",
    "\n",
    "# Create DataFrame with matched pairs\n",
    "matched_records = pd.DataFrame(matched_pairs, columns=['ID_1', 'ID_2'])\n",
    "\n",
    "# Save matched records to a CSV file\n",
    "matched_records.to_csv('matched_records.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the final code to use for dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recordlinkage\n",
    "import pandas as pd\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "df_usp_final = df_usp_final.fillna('')\n",
    "df_makg_final = df_makg_final.fillna('')\n",
    "\n",
    "# Convert columns to string type\n",
    "columns_to_convert = ['name', 'gender', 'country', 'city', 'affiliation']\n",
    "df_usp_final[columns_to_convert] = df_usp_final[columns_to_convert].astype(str)\n",
    "df_makg_final[columns_to_convert] = df_makg_final[columns_to_convert].astype(str)\n",
    "\n",
    "# Define indexing\n",
    "indexer = recordlinkage.Index()\n",
    "indexer.block('name')\n",
    "indexer.sortedneighbourhood('country')\n",
    "pairs = indexer.index(df_usp_final, df_makg_final)\n",
    "\n",
    "# Define comparison\n",
    "compare = recordlinkage.Compare()\n",
    "compare.string('name', 'name', method='jarowinkler', threshold=0.1)  # Adjust the threshold as needed\n",
    "compare.string('gender', 'gender', method='jarowinkler', threshold=0.1)\n",
    "compare.string('country', 'country', method='jarowinkler', threshold=0.1)\n",
    "compare.string('city', 'city', method='jarowinkler', threshold=0.1)\n",
    "compare.string('affiliation', 'affiliation', method='jarowinkler', threshold=0.1)\n",
    "similarity = compare.compute(pairs, df_usp_final, df_makg_final)\n",
    "\n",
    "# Perform classification\n",
    "matches = recordlinkage.KMeansClassifier().fit_predict(similarity)\n",
    "\n",
    "# Retrieve matched record pairs\n",
    "matched_indices = matches[matches.get_level_values(0) == 1].get_level_values(1).tolist()\n",
    "\n",
    "if len(matched_indices) > 0:\n",
    "    matched_records = []\n",
    "\n",
    "    for idx in matched_indices:\n",
    "        id_1 = pairs[idx][0]\n",
    "        id_2 = pairs[idx][1]\n",
    "        matched_records.append((id_1, id_2))\n",
    "\n",
    "    matched_records_df = pd.DataFrame(matched_records, columns=['ID_1', 'ID_2'])\n",
    "    matched_records_df.to_csv('matched_records.csv', index=False)\n",
    "else:\n",
    "    print(\"No matched pairs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "matched_records_df = pd.read_csv('matched_records.csv')\n",
    "\n",
    "# Fetch the matched records from the original dataframes\n",
    "for index, row in matched_records_df.iterrows():\n",
    "    index_1 = row['ID_1']\n",
    "    index_2 = row['ID_2']\n",
    "    record_1 = df_usp_final.iloc[index_1]\n",
    "    record_2 = df_makg_final.iloc[index_2]\n",
    "\n",
    "    # Print the matched records\n",
    "    print(f\"Matched Pair:\")\n",
    "    print(\"Record 1:\")\n",
    "    print(record_1)\n",
    "    print(\"Record 2:\")\n",
    "    print(record_2)\n",
    "    print(\"-------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
